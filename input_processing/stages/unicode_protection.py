"""
Unicode Attack Protection Module.

Provides comprehensive protection against Unicode-based attacks.
"""

import unicodedata


class UnicodeProtection:
    """Comprehensive Unicode attack protection."""

    # Dangerous Unicode categories
    DANGEROUS_CATEGORIES: set[str] = {
        "Cf",  # Format characters (invisible)
        "Co",  # Private use
        "Cn",  # Unassigned
    }

    # Specific dangerous characters
    DANGEROUS_CHARS: set[str] = {
        "\u200b",  # Zero-width space
        "\u200c",  # Zero-width non-joiner
        "\u200d",  # Zero-width joiner
        "\u200e",  # Left-to-right mark
        "\u200f",  # Right-to-left mark
        "\u202a",  # Left-to-right embedding
        "\u202b",  # Right-to-left embedding
        "\u202c",  # Pop directional formatting
        "\u202d",  # Left-to-right override
        "\u202e",  # Right-to-left override
        "\u2060",  # Word joiner
        "\ufeff",  # Zero-width no-break space
        "\u206a",  # Inhibit symmetric swapping
        "\u206b",  # Activate symmetric swapping
        "\u206c",  # Inhibit Arabic form shaping
        "\u206d",  # Activate Arabic form shaping
        "\u206e",  # National digit shapes
        "\u206f",  # Nominal digit shapes
        "\u2061",  # Function application
        "\u2062",  # Invisible times
        "\u2063",  # Invisible separator
        "\u2064",  # Invisible plus
        "\u2066",  # Left-to-right isolate
        "\u2067",  # Right-to-left isolate
        "\u2068",  # First strong isolate
        "\u2069",  # Pop directional isolate
        "\u180e",  # Mongolian vowel separator
        "\ufffe",  # Non-character
        "\uffff",  # Non-character
    }

    # Homograph mapping (common confusables)
    HOMOGRAPH_MAP: dict[str, str] = {
        # Cyrillic to Latin
        "Ð°": "a",
        "Ðµ": "e",
        "Ð¾": "o",
        "Ñ€": "p",
        "Ñ": "c",
        "Ñƒ": "y",
        "Ñ…": "x",
        "Ð": "A",
        "Ð’": "B",
        "Ð•": "E",
        "Ðš": "K",
        "Ðœ": "M",
        "Ð": "H",
        "Ðž": "O",
        "Ð ": "P",
        "Ð¡": "C",
        "Ð¢": "T",
        "Ð£": "Y",
        "Ð¥": "X",
        "Ñ•": "s",
        "Ñ–": "i",
        "Ñ˜": "j",
        "Ò»": "h",
        "Ô": "d",
        "Ô›": "q",
        "Ô": "w",
        "Ñ": "e",
        "Ñ‘": "e",
        # Greek to Latin
        "Î±": "a",
        "Î²": "b",
        "Î³": "y",
        "Î´": "d",
        "Îµ": "e",
        "Î¶": "z",
        "Î·": "n",
        "Î¸": "o",
        "Î¹": "i",
        "Îº": "k",
        "Î»": "l",
        "Î¼": "u",
        "Î½": "v",
        "Î¾": "e",
        "Î¿": "o",
        "Ï€": "n",
        "Ï": "p",
        "Ïƒ": "o",
        "Ï„": "t",
        "Ï…": "u",
        "Ï†": "o",
        "Ï‡": "x",
        "Ïˆ": "w",
        "Ï‰": "w",
        "Î‘": "A",
        "Î’": "B",
        "Î“": "T",
        "Î”": "A",
        "Î•": "E",
        "Î–": "Z",
        "Î—": "H",
        "Î˜": "O",
        "Î™": "I",
        "Îš": "K",
        "Î›": "A",
        "Îœ": "M",
        "Î": "N",
        "Îž": "E",
        "ÎŸ": "O",
        "Î ": "N",
        "Î¡": "P",
        "Î£": "E",
        "Î¤": "T",
        "Î¥": "Y",
        "Î¦": "O",
        "Î§": "X",
        "Î¨": "W",
        "Î©": "O",
        # Mathematical/Special to Latin
        "ð–º": "a",
        "ð–»": "b",
        "ð–¼": "c",
        "ð–½": "d",
        "ð–¾": "e",
        "ð–¿": "f",
        "ð—€": "g",
        "ð—": "h",
        "ð—‚": "i",
        "ð—ƒ": "j",
        "ð—„": "k",
        "ð—…": "l",
        "ð—†": "m",
        "ð—‡": "n",
        "ð—ˆ": "o",
        "ð—‰": "p",
        "ð—Š": "q",
        "ð—‹": "r",
        "ð—Œ": "s",
        "ð—": "t",
        "ð—Ž": "u",
        "ð—": "v",
        "ð—": "w",
        "ð—‘": "x",
        "ð—’": "y",
        "ð—“": "z",
        # Full-width to normal
        "ï½": "a",
        "ï½‚": "b",
        "ï½ƒ": "c",
        "ï½„": "d",
        "ï½…": "e",
        "ï½†": "f",
        "ï½‡": "g",
        "ï½ˆ": "h",
        "ï½‰": "i",
        "ï½Š": "j",
        "ï½‹": "k",
        "ï½Œ": "l",
        "ï½": "m",
        "ï½Ž": "n",
        "ï½": "o",
        "ï½": "p",
        "ï½‘": "q",
        "ï½’": "r",
        "ï½“": "s",
        "ï½”": "t",
        "ï½•": "u",
        "ï½–": "v",
        "ï½—": "w",
        "ï½˜": "x",
        "ï½™": "y",
        "ï½š": "z",
        "ï¼¡": "A",
        "ï¼¢": "B",
        "ï¼£": "C",
        "ï¼¤": "D",
        "ï¼¥": "E",
        "ï¼¦": "F",
        "ï¼§": "G",
        "ï¼¨": "H",
        "ï¼©": "I",
        "ï¼ª": "J",
        "ï¼«": "K",
        "ï¼¬": "L",
        "ï¼­": "M",
        "ï¼®": "N",
        "ï¼¯": "O",
        "ï¼°": "P",
        "ï¼±": "Q",
        "ï¼²": "R",
        "ï¼³": "S",
        "ï¼´": "T",
        "ï¼µ": "U",
        "ï¼¶": "V",
        "ï¼·": "W",
        "ï¼¸": "X",
        "ï¼¹": "Y",
        "ï¼º": "Z",
        "ï¼": "0",
        "ï¼‘": "1",
        "ï¼’": "2",
        "ï¼“": "3",
        "ï¼”": "4",
        "ï¼•": "5",
        "ï¼–": "6",
        "ï¼—": "7",
        "ï¼˜": "8",
        "ï¼™": "9",
        # Common ligatures
        "ï¬": "fi",
        "ï¬‚": "fl",
        "ï¬€": "ff",
        "ï¬ƒ": "ffi",
        "ï¬„": "ffl",
        "ï¬…": "st",
        "ï¬†": "st",
        # IPA Extensions
        "É‘": "a",  # Latin alpha
        "É": "a",  # Turned a
        "É’": "a",  # Turned alpha
        "Ê™": "b",  # Small capital B
        "É•": "c",  # Curled c
        "É–": "d",  # Retroflex d
        "É›": "e",  # Open e
        "Éœ": "e",  # Reversed open e
        "É¢": "g",  # Small capital G
        "É¦": "h",  # H with hook
        "Éª": "i",  # Small capital I
        "É¨": "i",  # Barred i
        "ÊŸ": "l",  # Small capital L
        "É´": "n",  # Small capital N
        "É”": "o",  # Open o
        "Éµ": "o",  # Barred o
        "Ê€": "r",  # Small capital R
        "Ê": "r",  # Inverted small capital R
        "Ê": "y",  # Small capital Y
        "ÊŠ": "u",  # Upsilon
    }

    # Script names that should not be mixed
    SCRIPT_NAMES: set[str] = {
        "LATIN",
        "CYRILLIC",
        "GREEK",
        "ARABIC",
        "HEBREW",
        "DEVANAGARI",
        "BENGALI",
        "GURMUKHI",
        "GUJARATI",
        "ORIYA",
        "TAMIL",
        "TELUGU",
        "KANNADA",
        "MALAYALAM",
        "SINHALA",
        "THAI",
        "LAO",
        "TIBETAN",
        "MYANMAR",
        "GEORGIAN",
        "HANGUL",
        "ETHIOPIC",
        "CHEROKEE",
        "CANADIAN",
        "OGHAM",
        "RUNIC",
        "TAGALOG",
        "HANUNOO",
        "BUHID",
        "TAGBANWA",
        "KHMER",
        "MONGOLIAN",
    }

    @staticmethod
    def normalize_unicode(text: str) -> str:
        """Normalize Unicode text to prevent attacks."""
        if not text:
            return text

        # Step 1: NFD normalization (decompose)
        text = unicodedata.normalize("NFD", text)

        # Step 2: Remove dangerous characters
        cleaned = []
        for char in text:
            if char not in UnicodeProtection.DANGEROUS_CHARS:
                cleaned.append(char)
        text = "".join(cleaned)

        # Step 3: Remove characters from dangerous categories
        cleaned = []
        for char in text:
            category = unicodedata.category(char)
            if category not in UnicodeProtection.DANGEROUS_CATEGORIES:
                cleaned.append(char)
        text = "".join(cleaned)

        # Step 4: Convert homographs
        chars = list(text)
        for i, char in enumerate(chars):
            if char in UnicodeProtection.HOMOGRAPH_MAP:
                chars[i] = UnicodeProtection.HOMOGRAPH_MAP[char]
        text = "".join(chars)

        # Step 5: NFC normalization (compose)
        text = unicodedata.normalize("NFC", text)

        # Step 6: Remove excessive combining characters
        # Allow max 2 combining chars per base character
        result = []
        combining_count = 0

        for char in text:
            if unicodedata.category(char).startswith("M"):
                # Combining character
                combining_count += 1
                if combining_count <= 2:
                    result.append(char)
            else:
                # Base character
                combining_count = 0
                result.append(char)

        return "".join(result)

    @staticmethod
    def detect_unicode_attack(text: str) -> bool:
        """Detect potential Unicode-based attacks."""
        if not text:
            return False

        # Check for dangerous characters
        for char in text:
            if char in UnicodeProtection.DANGEROUS_CHARS:
                return True

        # Check for mixed scripts
        scripts = set()
        for char in text:
            if char.isalpha():
                try:
                    char_name = unicodedata.name(char, "")
                    if char_name:
                        # Extract script from character name
                        for script in UnicodeProtection.SCRIPT_NAMES:
                            if script in char_name:
                                scripts.add(script)
                                break
                except ValueError:
                    pass

        # Multiple scripts = possible homograph attack
        if len(scripts) > 1:
            # Allow mixing with LATIN for legitimate cases
            non_latin_scripts = scripts - {"LATIN"}
            if len(non_latin_scripts) > 1:
                return True
            # If mixing Cyrillic/Greek with Latin, it's suspicious
            if non_latin_scripts and "LATIN" in scripts:
                suspicious_scripts = {"CYRILLIC", "GREEK"}
                if non_latin_scripts & suspicious_scripts:
                    return True

        # Check for excessive combining characters
        text_len = len(text)
        if text_len > 0:
            combining_count = sum(bool(unicodedata.category(c).startswith("M")) for c in text)

            if combining_count > text_len * 0.3:  # More than 30%
                return True

        # Check for right-to-left override
        rtl_chars = {"\u202e", "\u202b", "\u200f"}
        if any(char in text for char in rtl_chars):
            return True

        # Check for invisible characters
        invisible_count = sum(bool(unicodedata.category(c) in {"Cf", "Cc"}) for c in text)

        return invisible_count > 0

    @staticmethod
    def remove_bidi_controls(text: str) -> str:
        """Remove bidirectional control characters."""
        if not text:
            return text

        bidi_controls = {
            "\u200e",  # LRM
            "\u200f",  # RLM
            "\u202a",  # LRE
            "\u202b",  # RLE
            "\u202c",  # PDF
            "\u202d",  # LRO
            "\u202e",  # RLO
            "\u2066",  # LRI
            "\u2067",  # RLI
            "\u2068",  # FSI
            "\u2069",  # PDI
        }

        return "".join(c for c in text if c not in bidi_controls)

    @staticmethod
    def to_ascii_safe(text: str) -> str:
        """Convert to ASCII-safe representation."""
        if not text:
            return text

        # First normalize
        text = UnicodeProtection.normalize_unicode(text)

        # Then convert to ASCII
        try:
            # Try simple ASCII encode
            return text.encode("ascii", "ignore").decode("ascii")
        except Exception:
            # Fallback: convert character by character
            result = []
            for char in text:
                try:
                    char.encode("ascii")
                    result.append(char)
                except Exception:
                    # Try to get ASCII representation
                    if char in UnicodeProtection.HOMOGRAPH_MAP:
                        result.append(UnicodeProtection.HOMOGRAPH_MAP[char])
                    else:
                        # Skip non-ASCII chars
                        pass
            return "".join(result)

    @staticmethod
    def sanitize(text: str, allow_unicode: bool = True, max_length: int | None = None) -> str:
        """Main sanitization method."""
        if not text:
            return text

        # Always normalize
        text = UnicodeProtection.normalize_unicode(text)

        # Remove bidi controls
        text = UnicodeProtection.remove_bidi_controls(text)

        # Convert to ASCII if required
        if not allow_unicode:
            text = UnicodeProtection.to_ascii_safe(text)

        # Apply length limit if specified
        if max_length and len(text) > max_length:
            text = text[:max_length]

        return text.strip()
